dataset:
  root: <path_to_CSV_file>
  
  batch_size: 16

model:
  name: mi
  args:
    encoder:
      pretrain: true
      args:
        source: speechbrain/spkrec-ecapa-voxceleb
    classifier:
      name: mlp
      args:
        n_input: 192
        layers: [ 256, 256 ]
        activation: ReLU
        dropout: 0.2

optimizer:
  name: AdamW
  args:
    lr: 0.0001

epoch_max: 20
scheduler:
  name: CosineAnnealingLR
  args:
    T_max: 20
    eta_min: 0.00001

epoch_save: 10
precision: full
resume: save/_train_ecapa_mlp/epoch-10.pth
